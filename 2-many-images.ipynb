{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with many files\n",
    "\n",
    "In `1-singlefile.ipynb` we learned how to extract subsets and reproject a single image using a variety of tools (GDAL, rasterio, xarray, rioxarray, and holoviz). Often you want to work with a whole stack of imagery - for example let's see how to create a timeseries of backscatter over [Jakobshavn_Glacier](https://en.wikipedia.org/wiki/Jakobshavn_Glacier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounding box of interest \n",
    "# draw on here: http://geojson.io/\n",
    "import geopandas as gpd\n",
    "import geoviews as gv\n",
    "import hvplot.pandas\n",
    "gf = gpd.read_file('jakobshavn.geojson')\n",
    "tiles = gv.tile_sources.EsriTerrain\n",
    "bbox = gf.hvplot.polygons(alpha=0.2, geo=True)\n",
    "\n",
    "lonmin, latmin, lonmax, latmax = gf.bounds.values[0]\n",
    "print('bounding box=', lonmin, latmax, lonmax, latmin)\n",
    "\n",
    "tiles * bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get urls from CMR\n",
    "import cmr\n",
    "def get_cmr_urls():\n",
    "    short_name = 'NSIDC-0723'\n",
    "    version = '3'\n",
    "    time_start = '2010-01-01T00:00:00Z'\n",
    "    time_end = '2022-10-05T15:43:33Z' #some far off time in the future\n",
    "    #time_start = None\n",
    "    #time_end = None\n",
    "    #bounding_box = '-54.85,69.31,-52.18,70.26'\n",
    "    bounding_box = None\n",
    "    polygon = None\n",
    "    filename_filter = '*gamma0*'\n",
    "    #filename_filter = None\n",
    "\n",
    "    urls = cmr.get_urls(short_name, version, time_start, time_end, bounding_box, polygon, filename_filter)\n",
    "    cogs = [url for url in urls if url.endswith('tif')]\n",
    "    return cogs\n",
    "    \n",
    "assets = get_cmr_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: reading from NSIDC SERVER REQUIRES you have a ~/.netrc file \n",
    "# behind the scenes we're using GDAL to make requests, and we set some Env vars for performance\n",
    "#GDAL_DISABLE_READDIR_ON_OPEN=EMPTY_DIR GDAL_HTTP_COOKIEFILE=.urs_cookies GDAL_HTTP_COOKIEJAR=.urs_cookies\n",
    "\n",
    "import os\n",
    "env = dict(GDAL_DISABLE_READDIR_ON_OPEN='EMPTY_DIR', \n",
    "           GDAL_HTTP_COOKIEFILE='.urs_cookies',\n",
    "           GDAL_HTTP_COOKIEJAR='.urs_cookies',\n",
    "           GDAL_MAX_RAW_BLOCK_CACHE_SIZE='200000000',\n",
    "           GDAL_SWATH_SIZE='200000000',\n",
    "           VSI_CURL_CACHE_SIZE='200000000')\n",
    "os.environ.update(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using COG overviews is a great way to get a quick low-resolution view of the data\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import hvplot.xarray\n",
    "da = rioxarray.open_rasterio(asset_list[-1], overview_level=4, masked=True).squeeze('band') \n",
    "img = da.hvplot.image(cmap='gray', aspect='equal', frame_width=500)\n",
    "\n",
    "# convert our bounding box to epsg:3413 (south polar sterographic)\n",
    "gf3413 = gf.to_crs(3413)\n",
    "aoi = gf3413.hvplot.polygons(alpha=0.2, color='red', aspect='equal', frame_width=500)\n",
    "\n",
    "img *  aoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In many cases we're only interested in a small subset of big data (like that bounding box above!)\n",
    "#da.rio.clip(gf3413.geometry).hvplot.image(cmap='gray') #loads full raster\n",
    "#da.rio.clip_box(**gf3413.bounds).hvplot.image(cmap='gray') # more memory-efficent\n",
    "\n",
    "#new option in 0.2 release https://nbviewer.jupyter.org/github/corteva/rioxarray/blob/master/docs/examples/clip_geom.ipynb#Clipping-larger-rasters\n",
    "#da = rioxarray.open(url, masked=True).rio.clip(geometries, from_disk=True)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "#open subset directly\n",
    "da = rioxarray.open_rasterio(asset_list[0], masked=True).squeeze('band')\n",
    "subset = da.rio.clip_box(**gf3413.bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Will do this clipping taking advantage of multiple CPUs. In serial takes ~1.2 min\n",
    "# NOTE: it should be same time to load full datasets or pre-clip with rio.clip (maybe not if using rio.clip from _disk)\n",
    "# NOTE: using masked=True promotes float32 to float64 data\n",
    "\n",
    "\n",
    "import dask\n",
    "import pandas as pd\n",
    "\n",
    "@dask.delayed\n",
    "def lazy_open(href, masked=True):\n",
    "    filename = href.split('/')[-1] \n",
    "    date = href.split('/')[-2] \n",
    "    da = rioxarray.open_rasterio(href, chunks=(1, \"auto\", -1), masked=masked).rename(band='time') \n",
    "    da['time'] = [pd.to_datetime(date)]\n",
    "    da['filename'] = filename\n",
    "    return da\n",
    "\n",
    "# Seems single-machine scheduler uses threads by default (ThreadPool), you can use processes instead (ProcessPool)\n",
    "#with dask.config.set(scheduler='processes'): \n",
    "# NSIDC raises HTTP 503 for threads>15 it seems...\n",
    "with dask.config.set({'scheduler':'threads', 'num_workers':12}):\n",
    "    dataArrays = dask.compute(*[lazy_open(href, masked=False) for href in asset_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# NOTE: this is fast with dask arrays, can run out of memory with numpy arrays\n",
    "DA = xr.concat(dataArrays, dim='time', join='override', combine_attrs='drop').rio.clip_box(**gf3413.bounds)\n",
    "DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time \n",
    "\n",
    "# Drop scenes that are all nans in the bbox\n",
    "\n",
    "#test = DA.dropna('time', how='all') # da\n",
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finally lets use a video scrubber widget\n",
    "import panel as pn\n",
    "\n",
    "# note: delay 1000ms between scenes\n",
    "#player = pn.widgets.Player(name='time', loop_policy='once', interval=500)\n",
    "#pn.Row(video, widgets={'time':player})\n",
    "\n",
    "panel = DA.hvplot.image(x='x',y='y', \n",
    "                        rasterize=True,\n",
    "                        cmap='gray', clim=(-25,5),\n",
    "                        aspect='equal', frame_width=800,\n",
    "                        widget_type='scrubber', widget_location='bottom') \n",
    "\n",
    "#widget = panel[1][1][0] \n",
    "#widget.interval = 500ms default\n",
    "panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "subset = DA.to_dataset(name='gamma0').compute()\n",
    "# save with compression\n",
    "data_settings = {\"zlib\": True, \"dtype\":'float32', \"complevel\": 9}\n",
    "encoding_dict = dict(gamma0=data_settings)\n",
    "subset.to_netcdf('mysubset.nc', encoding=encoding_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
