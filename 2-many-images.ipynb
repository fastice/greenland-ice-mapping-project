{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with many files\n",
    "\n",
    "In `1-singlefile.ipynb` we learned how to extract subsets and reproject a single image using a variety of tools (GDAL, rasterio, xarray, rioxarray, and holoviz). Often you want to work with a whole stack of imagery - for example let's see how to create a timeseries of backscatter over [Jakobshavn_Glacier](https://en.wikipedia.org/wiki/Jakobshavn_Glacier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounding box of interest \n",
    "# draw on here: http://geojson.io/\n",
    "import geopandas as gpd\n",
    "import geoviews as gv\n",
    "import hvplot.pandas\n",
    "gf = gpd.read_file('jakobshavn.geojson')\n",
    "tiles = gv.tile_sources.EsriTerrain\n",
    "bbox = gf.hvplot.polygons(alpha=0.2, geo=True)\n",
    "\n",
    "lonmin, latmin, lonmax, latmax = gf.bounds.values[0]\n",
    "print('bounding box=', lonmin, latmax, lonmax, latmin)\n",
    "\n",
    "tiles * bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use NASA CMR STAC endpoint to get files\n",
    "\n",
    "# Can brows collecitons and catalogs by year\n",
    "# https://cmr.earthdata.nasa.gov/stac/NSIDC_ECS/collections/C1908075185-NSIDC_ECS/2015 \n",
    "\n",
    "# Search returns a single-file STAC of all the items\n",
    "#https://cmr.earthdata.nasa.gov/stac/NSIDC_ECS/search?collections=C1908075185-NSIDC_ECS\n",
    "\n",
    "import intake\n",
    "from satsearch import Search\n",
    "\n",
    "def get_STAC_items(url, collection, dates, bbox):\n",
    "    results = Search.search(url=url,\n",
    "                        collections=[collection], \n",
    "                        datetime=dates,\n",
    "                        bbox=bbox,    \n",
    "                        sortby=['+properties.datetime'])\n",
    "\n",
    "    items = results.items()\n",
    "    print(f'Found {len(items)} Items')\n",
    "    \n",
    "    return intake.open_stac_item_collection(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "URL='https://cmr.earthdata.nasa.gov/stac/NSIDC_ECS'\n",
    "bbox = [lonmin, latmin, lonmax, latmax] \n",
    "collection = 'C1908075185-NSIDC_ECS' # MEaSUREs Greenland Image Mosaics from Sentinel-1A and -1B V003\"\n",
    "dates = '2014-01-01/2020-12-31' \n",
    "items = get_STAC_items(URL, collection, dates, bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No great way to filter by asset type currently (gamma0, sigma0, thumbnail), so need a function\n",
    "def filter_assets(catalog, key='1', pattern='gamma0'):\n",
    "    \n",
    "    all_assets = [item.assets.get(key) for item in catalog._stac_obj]\n",
    "    all_hrefs = [asset.get('href') for asset in all_assets if asset] #ignore None values if asset key missing\n",
    "    filtered = [i for i in all_hrefs if pattern in i]\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "pattern = 'gamma0'\n",
    "asset_list = filter_assets(items, pattern='gamma0')\n",
    "print(f'Found {len(asset_list)} urls matching pattern {pattern}')\n",
    "asset_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: reading from NSIDC SERVER REQUIRES you have a ~/.netrc file \n",
    "# behind the scenes we're using GDAL to make requests, and we set some Env vars for performance\n",
    "#GDAL_DISABLE_READDIR_ON_OPEN=EMPTY_DIR GDAL_HTTP_COOKIEFILE=.urs_cookies GDAL_HTTP_COOKIEJAR=.urs_cookies\n",
    "\n",
    "import os\n",
    "env = dict(GDAL_DISABLE_READDIR_ON_OPEN='EMPTY_DIR', \n",
    "           GDAL_HTTP_COOKIEFILE='.urs_cookies',\n",
    "           GDAL_HTTP_COOKIEJAR='.urs_cookies',\n",
    "           GDAL_MAX_RAW_BLOCK_CACHE_SIZE='200000000',\n",
    "           GDAL_SWATH_SIZE='200000000',\n",
    "           VSI_CURL_CACHE_SIZE='200000000')\n",
    "os.environ.update(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using COG overviews is a great way to get a quick low-resolution view of the data\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import hvplot.xarray\n",
    "da = rioxarray.open_rasterio(asset_list[-1], overview_level=4, masked=True).squeeze('band') \n",
    "img = da.hvplot.image(cmap='gray', aspect='equal', frame_width=500)\n",
    "\n",
    "# convert our bounding box to epsg:3413 (south polar sterographic)\n",
    "gf3413 = gf.to_crs(3413)\n",
    "aoi = gf3413.hvplot.polygons(alpha=0.2, color='red', aspect='equal', frame_width=500)\n",
    "\n",
    "img *  aoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In many cases we're only interested in a small subset of big data (like that bounding box above!)\n",
    "#da.rio.clip(gf3413.geometry).hvplot.image(cmap='gray') #loads full raster\n",
    "#da.rio.clip_box(**gf3413.bounds).hvplot.image(cmap='gray') # more memory-efficent\n",
    "\n",
    "#new option in 0.2 release https://nbviewer.jupyter.org/github/corteva/rioxarray/blob/master/docs/examples/clip_geom.ipynb#Clipping-larger-rasters\n",
    "#da = rioxarray.open(url, masked=True).rio.clip(geometries, from_disk=True)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "#open subset directly\n",
    "da = rioxarray.open_rasterio(asset_list[0], masked=True).squeeze('band')\n",
    "subset = da.rio.clip_box(**gf3413.bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Will do this clipping taking advantage of multiple CPUs. In serial takes ~1.2 min\n",
    "# NOTE: it should be same time to load full datasets or pre-clip with rio.clip (maybe not if using rio.clip from _disk)\n",
    "# NOTE: using masked=True promotes float32 to float64 data\n",
    "\n",
    "\n",
    "import dask\n",
    "import pandas as pd\n",
    "\n",
    "@dask.delayed\n",
    "def lazy_open(href, masked=True):\n",
    "    filename = href.split('/')[-1] \n",
    "    date = href.split('/')[-2] \n",
    "    da = rioxarray.open_rasterio(href, chunks=(1, \"auto\", -1), masked=masked).rename(band='time') \n",
    "    da['time'] = [pd.to_datetime(date)]\n",
    "    da['filename'] = filename\n",
    "    return da\n",
    "\n",
    "# Seems single-machine scheduler uses threads by default (ThreadPool), you can use processes instead (ProcessPool)\n",
    "with dask.config.set(scheduler='processes'): \n",
    "    dataArrays = dask.compute(*[lazy_open(href, masked=False) for href in asset_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# NOTE: this is fast with dask arrays, can run out of memory with numpy arrays\n",
    "DA = xr.concat(dataArrays, dim='time', join='override', combine_attrs='drop').rio.clip_box(**gf3413.bounds)\n",
    "DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time \n",
    "\n",
    "# Drop scenes that are all nans in the bbox\n",
    "\n",
    "#test = DA.dropna('time', how='all') # da\n",
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finally lets use a video scrubber widget\n",
    "import panel as pn\n",
    "\n",
    "# note: delay 1000ms between scenes\n",
    "#player = pn.widgets.Player(name='time', loop_policy='once', interval=500)\n",
    "#pn.Row(video, widgets={'time':player})\n",
    "\n",
    "panel = DA.hvplot.image(x='x',y='y', \n",
    "                        rasterize=True,\n",
    "                        cmap='gray', clim=(-25,5),\n",
    "                        aspect='equal', frame_width=800,\n",
    "                        widget_type='scrubber', widget_location='bottom') \n",
    "\n",
    "#widget = panel[1][1][0] \n",
    "#widget.interval = 500ms default\n",
    "panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "subset = DA.to_dataset(name='gamma0').compute()\n",
    "# save with compression\n",
    "data_settings = {\"zlib\": True, \"dtype\":'float32', \"complevel\": 9}\n",
    "encoding_dict = dict(gamma0=data_settings)\n",
    "subset.to_netcdf('mysubset.nc', encoding=encoding_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
